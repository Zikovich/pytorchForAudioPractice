{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ScgEw-JA3cwK","executionInfo":{"status":"ok","timestamp":1670196381287,"user_tz":-60,"elapsed":24789,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"26d100e8-ff48-46fb-9d72-bccc83cd713f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/mygithub/pytorchForAudioPractice"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"auvJlBnuwn-7","executionInfo":{"status":"ok","timestamp":1670196381918,"user_tz":-60,"elapsed":637,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"0cfcaefe-7ef3-4077-b361-b1b9961baf09"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1j0tKfNJ74iEAtyLmY4PY6L2fktF9jL1I/mygithub/pytorchForAudioPractice\n"]}]},{"cell_type":"code","source":["!pip install torch torchaudio torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yi2Z0c4bXtfd","executionInfo":{"status":"ok","timestamp":1670196385913,"user_tz":-60,"elapsed":4000,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"8a46de7e-3f93-4ec1-9172-b4bf5df168bb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.12.1+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.13.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.1.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.23.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.9.24)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.0.4)\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import torch.autograd.profiler as profiler"],"metadata":{"id":"s8_9KJ0AYQ7F","executionInfo":{"status":"ok","timestamp":1670196388805,"user_tz":-60,"elapsed":2898,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def download_mnist_datasets():\n","  train_data = datasets.MNIST(train=True, download=True, transform=ToTensor(), root=\"data\")\n","  validate_data = datasets.MNIST(train=False, download=True, transform=ToTensor(), root=\"data\")\n","  return train_data, validate_data\n"],"metadata":{"id":"nI8j0ZW9X3W4","executionInfo":{"status":"ok","timestamp":1670170384155,"user_tz":-60,"elapsed":9,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from torch.nn.modules.linear import Linear\n","class FeedForwardNN(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.flatten = nn.Flatten()\n","    self.denseLayers = nn.Sequential(\n","        nn.Linear(28*28,256), # image size 28x28\n","        nn.ReLU(),\n","        nn.Linear(256,10) # 10 classes 0 to 9 \n","    )\n","    self.activation = nn.Softmax(dim=1)\n","\n","  def forward(self, inputData):\n","\n","    flattenedData = self.flatten(inputData)\n","    logits = self.denseLayers(flattenedData)\n","    predictions = self.activation(logits)\n","\n","    return predictions"],"metadata":{"id":"-z_FyG7fjjUG","executionInfo":{"status":"ok","timestamp":1670170384156,"user_tz":-60,"elapsed":9,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def trainOneEpoch(model, dataLoader, lossFn, optimiser, device):\n","\n","\n","  for input, target in dataLoader:\n","\n","    input, target = input.to(device), target.to(device)\n","\n","    #calculate loss\n","    predication = model(input)\n","\n","    loss = lossFn(predication, target)\n","\n","    # backpropagation and weight update\n","\n","    optimiser.zero_grad()\n","    loss.backward()\n","    optimiser.step()\n","\n","  print(f\"loss: {loss.item()}\")"],"metadata":{"id":"1l0qUQX1jp4I","executionInfo":{"status":"ok","timestamp":1670196388806,"user_tz":-60,"elapsed":12,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def train(model, dataLoader, lossFn, optimiser, device, epochs):\n","  for i in range(epochs):\n","\n","    print(f\"Epoch {i+1}\")\n","    trainOneEpoch(model, dataLoader, lossFn, optimiser, device)\n","    print(\"---------------------------\")\n","  \n","  print(\"Finished training\")"],"metadata":{"id":"JUEe9vv7oeFz","executionInfo":{"status":"ok","timestamp":1670196388807,"user_tz":-60,"elapsed":12,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# **Model Train**"],"metadata":{"id":"rXT3qou62IWm"}},{"cell_type":"code","source":["BATCH_SIZE = 128\n","LEARNING_RATE = 0.001\n","EPOCHS = 10\n","\n","# Download data\n","train_data, _ = download_mnist_datasets()\n","\n","# data load\n","dataLoader = DataLoader(train_data, batch_size=BATCH_SIZE)\n","\n","# Build Model\n","\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","else:\n","    device = \"cpu\"\n","\n","print(f\"Using {device}\")\n","\n","model = FeedForwardNN().to(device)\n","\n","# Train\n","\n","lossFn = nn.CrossEntropyLoss()\n","optimiser = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","\n","train(model, dataLoader, lossFn, optimiser, device, EPOCHS)\n","\n","# Save Model\n","\n","torch.save(model.state_dict(),\"modelNN.pth\")\n","print(\"Trained feed forward net saved at modelNN.pth\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mYYtuBibjqR5","executionInfo":{"status":"ok","timestamp":1670069895079,"user_tz":-60,"elapsed":72657,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"a259acec-75e3-4efd-90b1-d73981561ec6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu\n","Epoch 1\n","loss: 1.5117287635803223\n","---------------------------\n","Epoch 2\n","loss: 1.49992835521698\n","---------------------------\n","Epoch 3\n","loss: 1.4886845350265503\n","---------------------------\n","Epoch 4\n","loss: 1.4811148643493652\n","---------------------------\n","Epoch 5\n","loss: 1.475595474243164\n","---------------------------\n","Epoch 6\n","loss: 1.473410964012146\n","---------------------------\n","Epoch 7\n","loss: 1.4738837480545044\n","---------------------------\n","Epoch 8\n","loss: 1.4731812477111816\n","---------------------------\n","Epoch 9\n","loss: 1.4724453687667847\n","---------------------------\n","Epoch 10\n","loss: 1.4728208780288696\n","---------------------------\n","Finished training\n","Trained feed forward net saved at modelNN.pth\n"]}]},{"cell_type":"markdown","source":["# Model Infere"],"metadata":{"id":"H_VwYXy32STD"}},{"cell_type":"code","source":["CLASS_MAPPING = [\n","    \"0\",\n","    \"1\",\n","    \"2\",\n","    \"3\",\n","    \"4\",\n","    \"5\",\n","    \"6\",\n","    \"7\",\n","    \"8\",\n","    \"9\"\n","]"],"metadata":{"id":"a5PkQOx75Dcu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(model, input, target, classMapping):\n","  # switch eval \n","  model.eval()\n","\n","  # run infere without gradient evaluation\n","  with torch.no_grad():\n","    predictions = model(input)\n","\n","    # Tensor (1, 10) --> [[0.1, 0.001, .... , 0.8]]\n","    predictedIndex = predictions[0].argmax(0)\n","\n","    predicted = classMapping[predictedIndex]\n","    expected = classMapping[target]\n","\n","  return  predicted, expected"],"metadata":{"id":"yZztm4h04p0g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load Validate Data\n","_, validatingData = download_mnist_datasets()\n","\n","# Load Model\n","\n","feedForwardNetObj = FeedForwardNN()\n","\n","dictStatLoaded = torch.load(\"modelNN.pth\")\n","\n","feedForwardNetObj.load_state_dict(dictStatLoaded)\n","\n","# get sample from the validating data\n","input, target = validatingData[0][0], validatingData[0][1]\n","\n","# Make inference\n","\n","predicted, expected = predict(feedForwardNetObj, input, target, CLASS_MAPPING)\n","\n","print(f\"Predicted: '{predicted}', expected: '{expected}'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1njFmwQEPybI","executionInfo":{"status":"ok","timestamp":1670070165439,"user_tz":-60,"elapsed":247,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"ccddb651-cf76-441c-d110-ab55e02f4d29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted: '7', expected: '7'\n"]}]},{"cell_type":"markdown","source":["# Customized Urban Dataset\n","\n","https://urbansounddataset.weebly.com/urbansound8k.html"],"metadata":{"id":"DuIStHKi4GRP"}},{"cell_type":"markdown","source":["# To download dataset\n","\n","```\n","cd ---> path you need\n","!pip install opendatasets --upgrade --quiet\n","\n","import opendatasets as od\n","\n","dataset_url = 'https://goo.gl/8hY5ER'\n","od.download(dataset_url)\n","```\n","\n","ref: https://jovian.ai/charmzshab/urban-sound-dataset\n","\n","and also: https://www.kaggle.com/datasets/chrisfilo/urbansound8k"],"metadata":{"id":"6vCf2gSyFBcF"}},{"cell_type":"markdown","source":["# Use Mel Spectrogram as transformer"],"metadata":{"id":"CVf6ra7aEMFV"}},{"cell_type":"code","source":["from pandas.core.array_algos import transforms\n","import os\n","from torch.utils.data import Dataset\n","import pandas as pd\n","import torchaudio\n","\n","\n","class UrbanSoundDataset(Dataset):\n","\n","  def __init__(self, annotationsFile,\n","               audioDir, transformer,\n","               sampleRate, sampleLength,\n","               device):\n","    self.annotations = pd.read_csv(annotationsFile)\n","    self.audioDir = audioDir\n","    self.device = device\n","    self.transformer = transformer.to(self.device)\n","    self.targetSampleRate = sampleRate\n","    self.sampleLength = sampleLength\n","\n","\n","  def __len__(self):\n","    return len(self.annotations)\n","    \n","  def __getitem__(self,index):\n","    audioSamplePath = self._getAudioSamplePath(index)\n","    label = self._getAudioSampleLabel(index)\n","    signal, sr = torchaudio.load(audioSamplePath)\n","    signal = signal.to(self.device)\n","    # need to resample if necessary\n","    signal = self._resampleIfNecessary(signal, sr)\n","    # need to mix the channels in case of stero or multi channel\n","    signal = self._mixChannelIfNecessary(signal)\n","    # need to check the size of the sample so that right padding added or trunked\n","    signal = self._adjustSampleLength(signal)\n","    # need to transform signal to the transforemer\n","    signal = self.transformer(signal)\n","    return signal, label\n","\n","  def _getAudioSamplePath(self, index):\n","    fold = f\"fold{self.annotations.iloc[index,5]}\" # 5 where fold is located he cloumb # 5\n","    path = os.path.join(self.audioDir, fold, self.annotations.iloc[index, 0]) # where 0 is the raw of the .wav files names\n","    return path\n","\n","  def _getAudioSampleLabel(self, index):\n","    return self.annotations.iloc[index, 6] # where the raw #6 is the classId\n","\n","  def _resampleIfNecessary(self, signal, sr):\n","    if sr != self.targetSampleRate:\n","      resampler = torchaudio.transforms.Resample(sr, self.targetSampleRate).to(self.device)     \n","      signal = resampler(signal)\n","    return signal\n","\n","  def _mixChannelIfNecessary(self, signal):\n","    # example, if signal is stero so its shape will be (2,16000)\n","    if signal.shape[0] > 1:\n","      signal = torch.mean(signal, dim=0, keepdim=True)\n","    return signal\n","\n","  def _adjustSampleLength(self, signal):\n","    signalLen = signal.shape[1]\n","    # check in case of small sample so needed padding to be added\n","    if signalLen < self.sampleLength:\n","      numMissingSample = self.sampleLength - signalLen\n","      lastDimPadding = (0, numMissingSample)\n","      signal = torch.nn.functional.pad(signal, lastDimPadding)\n","    \n","    # in case of signal len greater than required len\n","    elif signalLen > self.sampleLength:\n","      signal = signal[:, :self.sampleLength]\n","\n","    return signal\n"],"metadata":{"id":"I5wFi2J05_Kq","executionInfo":{"status":"ok","timestamp":1670196399894,"user_tz":-60,"elapsed":951,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"Tn0amUUAVYTW","executionInfo":{"status":"ok","timestamp":1670194239643,"user_tz":-60,"elapsed":9,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"aa429df6-51fa-49fc-e73e-91a0adf0b833"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/drive/.shortcut-targets-by-id/1j0tKfNJ74iEAtyLmY4PY6L2fktF9jL1I/mygithub/pytorchForAudioPractice'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["from torch.cuda import is_available\n","ANNOTATION_FILE = \"/content/drive/MyDrive/mygithub/pytorchForAudioPractice/data/UrbanSound8K/metadata/UrbanSound8K.csv\"\n","AUDIO_DIR = \"/content/drive/MyDrive/mygithub/pytorchForAudioPractice/data/UrbanSound8K/audio\"\n","SAMPLE_RATE = 22050\n","NUM_SAMPLE = 22050\n","\n","melSpectrogram = torchaudio.transforms.MelSpectrogram(\n","    sample_rate=SAMPLE_RATE,\n","    n_fft=1024,\n","    hop_length=512,\n","    n_mels=64)\n","\n","if torch.cuda.is_available():\n","  device = \"cuda\"\n","else:\n","  device = \"cpu\"\n","print(f\"Using device {device}\")\n","usd = UrbanSoundDataset(ANNOTATION_FILE,\n","                        AUDIO_DIR,\n","                        melSpectrogram,\n","                        SAMPLE_RATE,\n","                        NUM_SAMPLE,\n","                        device)\n","\n","print(f\"There are {len(usd)} samples in the dataset.\")\n","signal, label = usd[0]\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IeP0godK5FyB","executionInfo":{"status":"ok","timestamp":1670195270277,"user_tz":-60,"elapsed":6404,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"911abc78-6f61-4aa7-c7f2-bc3c41c631e2"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device cuda\n","There are 8732 samples in the dataset.\n"," device = cuda\n"]}]},{"cell_type":"code","source":["label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PyEbGY9sElK8","executionInfo":{"status":"ok","timestamp":1670195275906,"user_tz":-60,"elapsed":426,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"4048a662-1039-4518-8620-e9928b21cb7f"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["signal.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Uu4417MEGIA","executionInfo":{"status":"ok","timestamp":1670195278174,"user_tz":-60,"elapsed":7,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"272c565b-305c-46fa-9618-e056ed5f1cd1"},"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 64, 44])"]},"metadata":{},"execution_count":21}]},{"cell_type":"markdown","source":["# CNN For Audio Classification"],"metadata":{"id":"b0zWZMWGiGQq"}},{"cell_type":"code","source":["from torch import nn\n","from torchsummary import summary\n","\n","class CNNNetwork(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    # 4 conv blocks / flatten / linear / softmax\n","    self.conv1 = nn.Sequential(\n","        nn.Conv2d(\n","            in_channels=1,\n","            out_channels=16,\n","            kernel_size=3,\n","            stride=1,\n","            padding=2\n","        ),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2)\n","    )\n","    self.conv2 = nn.Sequential(\n","        nn.Conv2d(\n","            in_channels=16,\n","            out_channels=32,\n","            kernel_size=3,\n","            stride=1,\n","            padding=2\n","        ),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2)\n","    )\n","    self.conv3 = nn.Sequential(\n","        nn.Conv2d(\n","            in_channels=32,\n","            out_channels=64,\n","            kernel_size=3,\n","            stride=1,\n","            padding=2\n","        ),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2)\n","    )\n","    self.conv4 = nn.Sequential(\n","        nn.Conv2d(\n","            in_channels=64,\n","            out_channels=128,\n","            kernel_size=3,\n","            stride=1,\n","            padding=2\n","        ),\n","        nn.ReLU(),\n","        nn.MaxPool2d(kernel_size=2)\n","    )\n","    self.flatten = nn.Flatten()\n","    self.linear = nn.Linear(128*5*4, 10) # 128*5*4 is the out of the flatten, 10 is the classes of urban sound\n","    self.softmax = nn.Softmax(dim=1)\n","  \n","  def forward(self, inputData):\n","    x = self.conv1(inputData)\n","    x = self.conv2(x)\n","    x = self.conv3(x)\n","    x = self.conv4(x)\n","    x = self.flatten(x)\n","    logits = self.linear(x)\n","    predictions = self.softmax(logits)\n","    return predictions\n","\n"],"metadata":{"id":"9vbtjXU8oN05","executionInfo":{"status":"ok","timestamp":1670196412277,"user_tz":-60,"elapsed":372,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["cnn = CNNNetwork()\n","summary(cnn.to(device), (1,64,44))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EuOQTEf0q-h7","executionInfo":{"status":"ok","timestamp":1670195357579,"user_tz":-60,"elapsed":17,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"e587aaf5-d525-43fa-89de-3772769e7bba"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1           [-1, 16, 66, 46]             160\n","              ReLU-2           [-1, 16, 66, 46]               0\n","         MaxPool2d-3           [-1, 16, 33, 23]               0\n","            Conv2d-4           [-1, 32, 35, 25]           4,640\n","              ReLU-5           [-1, 32, 35, 25]               0\n","         MaxPool2d-6           [-1, 32, 17, 12]               0\n","            Conv2d-7           [-1, 64, 19, 14]          18,496\n","              ReLU-8           [-1, 64, 19, 14]               0\n","         MaxPool2d-9             [-1, 64, 9, 7]               0\n","           Conv2d-10           [-1, 128, 11, 9]          73,856\n","             ReLU-11           [-1, 128, 11, 9]               0\n","        MaxPool2d-12            [-1, 128, 5, 4]               0\n","          Flatten-13                 [-1, 2560]               0\n","           Linear-14                   [-1, 10]          25,610\n","          Softmax-15                   [-1, 10]               0\n","================================================================\n","Total params: 122,762\n","Trainable params: 122,762\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 1.83\n","Params size (MB): 0.47\n","Estimated Total Size (MB): 2.31\n","----------------------------------------------------------------\n"]}]},{"cell_type":"markdown","source":["# Train CNN\n"],"metadata":{"id":"rE9J_MpgrH1R"}},{"cell_type":"code","source":["from torch.cuda import is_available\n","\n","ANNOTATION_FILE = \"/content/drive/MyDrive/mygithub/pytorchForAudioPractice/data/UrbanSound8K/metadata/UrbanSound8K.csv\"\n","AUDIO_DIR = \"/content/drive/MyDrive/mygithub/pytorchForAudioPractice/data/UrbanSound8K/audio\"\n","\n","SAMPLE_RATE = 22050\n","NUM_SAMPLE = 22050\n","\n","BATCH_SIZE = 128\n","EPOCHS = 10\n","LEARNING_RATE = 0.001\n","\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","else:\n","    device = \"cpu\"\n","print(f\"Using {device}\")\n","\n","# instantiating our dataset object and create data loader\n","mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n","    sample_rate=SAMPLE_RATE,\n","    n_fft=1024,\n","    hop_length=512,\n","    n_mels=64\n",")\n","\n","usd = UrbanSoundDataset(ANNOTATION_FILE,\n","                        AUDIO_DIR,\n","                        mel_spectrogram,\n","                        SAMPLE_RATE,\n","                        NUM_SAMPLE,\n","                        device)\n","\n","train_dataloader = DataLoader(usd, BATCH_SIZE)\n","\n","# construct model and assign it to device\n","cnn = CNNNetwork().to(device)\n","print(cnn)\n","\n","# initialise loss funtion + optimiser\n","loss_fn = nn.CrossEntropyLoss()\n","optimiser = torch.optim.Adam(cnn.parameters(),\n","                              lr=LEARNING_RATE)\n","\n","# train model\n","train(cnn, train_dataloader, loss_fn, optimiser, device, EPOCHS)\n","\n","# save model\n","torch.save(cnn.state_dict(), \"cnn.pth\")\n","print(\"Trained feed forward net saved at feedforwardnet.pth\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t-JQjrwa40_k","executionInfo":{"status":"ok","timestamp":1670205600532,"user_tz":-60,"elapsed":9183175,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"5248c1a9-0497-4447-d130-2e07a1db0b18"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu\n","CNNNetwork(\n","  (conv1): Sequential(\n","    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv2): Sequential(\n","    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv3): Sequential(\n","    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (conv4): Sequential(\n","    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  )\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear): Linear(in_features=2560, out_features=10, bias=True)\n","  (softmax): Softmax(dim=1)\n",")\n","Epoch 1\n","loss: 2.431323766708374\n","---------------------------\n","Epoch 2\n","loss: 2.4091808795928955\n","---------------------------\n","Epoch 3\n","loss: 2.26098895072937\n","---------------------------\n","Epoch 4\n","loss: 2.2916336059570312\n","---------------------------\n","Epoch 5\n","loss: 2.2841575145721436\n","---------------------------\n","Epoch 6\n","loss: 2.2849082946777344\n","---------------------------\n","Epoch 7\n","loss: 2.3633060455322266\n","---------------------------\n","Epoch 8\n","loss: 2.197097063064575\n","---------------------------\n","Epoch 9\n","loss: 2.1045420169830322\n","---------------------------\n","Epoch 10\n","loss: 2.2210323810577393\n","---------------------------\n","Finished training\n","Trained feed forward net saved at feedforwardnet.pth\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"78JinNu_6hJg"},"execution_count":null,"outputs":[]}]}