{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ScgEw-JA3cwK","executionInfo":{"status":"ok","timestamp":1670163876141,"user_tz":-60,"elapsed":2695,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"c31a7037-0262-4a1c-83cf-5d09154136bb"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["cd /content/drive/MyDrive/mygithub/pytorchForAudioPractice"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"auvJlBnuwn-7","executionInfo":{"status":"ok","timestamp":1670163876143,"user_tz":-60,"elapsed":13,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"174e5bfa-e016-4fb0-9fc8-f28283bdb55c"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/1j0tKfNJ74iEAtyLmY4PY6L2fktF9jL1I/mygithub/pytorchForAudioPractice\n"]}]},{"cell_type":"code","source":["!pip install torch torchaudio torchvision"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yi2Z0c4bXtfd","executionInfo":{"status":"ok","timestamp":1670163879072,"user_tz":-60,"elapsed":2938,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"ad457936-320c-4428-e43b-8bc77f9a6529"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.8/dist-packages (1.12.1+cu113)\n","Requirement already satisfied: torchaudio in /usr/local/lib/python3.8/dist-packages (0.12.1+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (0.13.1+cu113)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch) (4.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from torchvision) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision) (7.1.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->torchvision) (2.10)\n"]}]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torchvision import datasets\n","from torchvision.transforms import ToTensor\n","import torch.autograd.profiler as profiler"],"metadata":{"id":"s8_9KJ0AYQ7F","executionInfo":{"status":"ok","timestamp":1670163879073,"user_tz":-60,"elapsed":12,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def download_mnist_datasets():\n","  train_data = datasets.MNIST(train=True, download=True, transform=ToTensor(), root=\"data\")\n","  validate_data = datasets.MNIST(train=False, download=True, transform=ToTensor(), root=\"data\")\n","  return train_data, validate_data\n"],"metadata":{"id":"nI8j0ZW9X3W4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn.modules.linear import Linear\n","class FeedForwardNN(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    self.flatten = nn.Flatten()\n","    self.denseLayers = nn.Sequential(\n","        nn.Linear(28*28,256), # image size 28x28\n","        nn.ReLU(),\n","        nn.Linear(256,10) # 10 classes 0 to 9 \n","    )\n","    self.activation = nn.Softmax(dim=1)\n","\n","  def forward(self, inputData):\n","\n","    flattenedData = self.flatten(inputData)\n","    logits = self.denseLayers(flattenedData)\n","    predictions = self.activation(logits)\n","\n","    return predictions"],"metadata":{"id":"-z_FyG7fjjUG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def trainOneEpoch(model, dataLoader, lossFn, optimiser, device):\n","\n","\n","  for input, target in dataLoader:\n","\n","    input, target = input.to(device), target.to(device)\n","\n","    #calculate loss\n","    predication = model(input)\n","\n","    loss = lossFn(predication, target)\n","\n","    # backpropagation and weight update\n","\n","    optimiser.zero_grad()\n","    loss.backward()\n","    optimiser.step()\n","\n","  print(f\"loss: {loss.item()}\")"],"metadata":{"id":"1l0qUQX1jp4I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, dataLoader, lossFn, optimiser, device, epochs):\n","  for i in range(epochs):\n","\n","    print(f\"Epoch {i+1}\")\n","    trainOneEpoch(model, dataLoader, lossFn, optimiser, device)\n","    print(\"---------------------------\")\n","  \n","  print(\"Finished training\")"],"metadata":{"id":"JUEe9vv7oeFz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Model Train**"],"metadata":{"id":"rXT3qou62IWm"}},{"cell_type":"code","source":["BATCH_SIZE = 128\n","LEARNING_RATE = 0.001\n","EPOCHS = 10\n","\n","# Download data\n","train_data, _ = download_mnist_datasets()\n","\n","# data load\n","dataLoader = DataLoader(train_data, batch_size=BATCH_SIZE)\n","\n","# Build Model\n","\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","else:\n","    device = \"cpu\"\n","\n","print(f\"Using {device}\")\n","\n","model = FeedForwardNN().to(device)\n","\n","# Train\n","\n","lossFn = nn.CrossEntropyLoss()\n","optimiser = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n","\n","\n","train(model, dataLoader, lossFn, optimiser, device, EPOCHS)\n","\n","# Save Model\n","\n","torch.save(model.state_dict(),\"modelNN.pth\")\n","print(\"Trained feed forward net saved at modelNN.pth\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mYYtuBibjqR5","executionInfo":{"status":"ok","timestamp":1670069895079,"user_tz":-60,"elapsed":72657,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"a259acec-75e3-4efd-90b1-d73981561ec6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cpu\n","Epoch 1\n","loss: 1.5117287635803223\n","---------------------------\n","Epoch 2\n","loss: 1.49992835521698\n","---------------------------\n","Epoch 3\n","loss: 1.4886845350265503\n","---------------------------\n","Epoch 4\n","loss: 1.4811148643493652\n","---------------------------\n","Epoch 5\n","loss: 1.475595474243164\n","---------------------------\n","Epoch 6\n","loss: 1.473410964012146\n","---------------------------\n","Epoch 7\n","loss: 1.4738837480545044\n","---------------------------\n","Epoch 8\n","loss: 1.4731812477111816\n","---------------------------\n","Epoch 9\n","loss: 1.4724453687667847\n","---------------------------\n","Epoch 10\n","loss: 1.4728208780288696\n","---------------------------\n","Finished training\n","Trained feed forward net saved at modelNN.pth\n"]}]},{"cell_type":"markdown","source":["# Model Infere"],"metadata":{"id":"H_VwYXy32STD"}},{"cell_type":"code","source":["CLASS_MAPPING = [\n","    \"0\",\n","    \"1\",\n","    \"2\",\n","    \"3\",\n","    \"4\",\n","    \"5\",\n","    \"6\",\n","    \"7\",\n","    \"8\",\n","    \"9\"\n","]"],"metadata":{"id":"a5PkQOx75Dcu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(model, input, target, classMapping):\n","  # switch eval \n","  model.eval()\n","\n","  # run infere without gradient evaluation\n","  with torch.no_grad():\n","    predictions = model(input)\n","\n","    # Tensor (1, 10) --> [[0.1, 0.001, .... , 0.8]]\n","    predictedIndex = predictions[0].argmax(0)\n","\n","    predicted = classMapping[predictedIndex]\n","    expected = classMapping[target]\n","\n","  return  predicted, expected"],"metadata":{"id":"yZztm4h04p0g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load Validate Data\n","_, validatingData = download_mnist_datasets()\n","\n","# Load Model\n","\n","feedForwardNetObj = FeedForwardNN()\n","\n","dictStatLoaded = torch.load(\"modelNN.pth\")\n","\n","feedForwardNetObj.load_state_dict(dictStatLoaded)\n","\n","# get sample from the validating data\n","input, target = validatingData[0][0], validatingData[0][1]\n","\n","# Make inference\n","\n","predicted, expected = predict(feedForwardNetObj, input, target, CLASS_MAPPING)\n","\n","print(f\"Predicted: '{predicted}', expected: '{expected}'\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1njFmwQEPybI","executionInfo":{"status":"ok","timestamp":1670070165439,"user_tz":-60,"elapsed":247,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"ccddb651-cf76-441c-d110-ab55e02f4d29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted: '7', expected: '7'\n"]}]},{"cell_type":"markdown","source":["# Customized Urban Dataset\n","\n","https://urbansounddataset.weebly.com/urbansound8k.html"],"metadata":{"id":"DuIStHKi4GRP"}},{"cell_type":"markdown","source":["# To download dataset\n","\n","```\n","cd ---> path you need\n","!pip install opendatasets --upgrade --quiet\n","\n","import opendatasets as od\n","\n","dataset_url = 'https://goo.gl/8hY5ER'\n","od.download(dataset_url)\n","```\n","\n","ref: https://jovian.ai/charmzshab/urban-sound-dataset\n","\n","and also: https://www.kaggle.com/datasets/chrisfilo/urbansound8k"],"metadata":{"id":"6vCf2gSyFBcF"}},{"cell_type":"markdown","source":["# Use Mel Spectrogram as transformer"],"metadata":{"id":"CVf6ra7aEMFV"}},{"cell_type":"code","source":["from pandas.core.array_algos import transforms\n","import os\n","from torch.utils.data import Dataset\n","import pandas as pd\n","import torchaudio\n","\n","\n","class UrbanSoundDataset(Dataset):\n","\n","  def __init__(self, annotationsFile, audioDir, transformer, sampleRate, sampleLength):\n","    self.annotations = pd.read_csv(annotationsFile)\n","    self.audioDir = audioDir\n","    self.transformer = transformer\n","    self.targetSampleRate = sampleRate\n","    self.sampleLength = sampleLength\n","\n","  def __len__(self):\n","    return len(self.annotations)\n","    \n","  def __getitem__(self,index):\n","    audioSamplePath = self._getAudioSamplePath(index)\n","    label = self._getAudioSampleLabel(index)\n","    signal, sr = torchaudio.load(audioSamplePath)\n","\n","    # need to resample if necessary\n","    signal = self._resampleIfNecessary(signal, sr)\n","    # need to mix the channels in case of stero or multi channel\n","    signal = self._mixChannelIfNecessary(signal)\n","    # need to check the size of the sample so that right padding added or trunked\n","    signal = self._adjustSampleLength(signal)\n","    # need to transform signal to the transforemer\n","    signal = self.transformer(signal)\n","    return signal, label\n","\n","  def _getAudioSamplePath(self, index):\n","    fold = f\"fold{self.annotations.iloc[index,5]}\" # 5 where fold is located he cloumb # 5\n","    path = os.path.join(self.audioDir, fold, self.annotations.iloc[index, 0]) # where 0 is the raw of the .wav files names\n","    return path\n","\n","  def _getAudioSampleLabel(self, index):\n","    return self.annotations.iloc[index, 6] # where the raw #6 is the classId\n","\n","  def _resampleIfNecessary(self, signal, sr):\n","    if sr != self.targetSampleRate:\n","      resampler = torchaudio.transforms.Resample(sr, self.targetSampleRate)\n","      signal = resampler(signal)\n","    return signal\n","\n","  def _mixChannelIfNecessary(self, signal):\n","    # example, if signal is stero so its shape will be (2,16000)\n","    if signal.shape[0] > 1:\n","      signal = torch.mean(signal, dim=0, keepdim=True)\n","    return signal\n","\n","  def _adjustSampleLength(self, signal):\n","    signalLen = signal.shape[1]\n","    # check in case of small sample so needed padding to be added\n","    if signalLen < self.sampleLength:\n","      numMissingSample = self.sampleLength - signalLen\n","      lastDimPadding = (0, numMissingSample)\n","      signal = torch.nn.functional.pad(signal, lastDimPadding)\n","    \n","    # in case of signal len greater than required len\n","    elif signalLen > self.sampleLength:\n","      signal = signal[:, :self.sampleLength]\n","\n","    return signal\n"],"metadata":{"id":"I5wFi2J05_Kq","executionInfo":{"status":"ok","timestamp":1670164006185,"user_tz":-60,"elapsed":981,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["ANNOTATION_FILE = \"/content/drive/MyDrive/mygithub/pytorchForAudioPractice/data/UrbanSound8K/metadata/UrbanSound8K.csv\"\n","AUDIO_DIR = \"/content/drive/MyDrive/mygithub/pytorchForAudioPractice/data/UrbanSound8K/audio\"\n","SAMPLE_RATE = 22050\n","NUM_SAMPLE = 22050\n","\n","melSpectrogram = torchaudio.transforms.MelSpectrogram(\n","    sample_rate=SAMPLE_RATE,\n","    n_fft=1024,\n","    hop_length=512,\n","    n_mels=64)\n","\n","usd = UrbanSoundDataset(ANNOTATION_FILE, AUDIO_DIR, melSpectrogram, SAMPLE_RATE, NUM_SAMPLE)\n","\n","print(f\"There are {len(usd)} samples in the dataset.\")\n","signal, label = usd[0]\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IeP0godK5FyB","executionInfo":{"status":"ok","timestamp":1670164006595,"user_tz":-60,"elapsed":6,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"e7a442eb-9214-4a8c-c058-6cd60059816a"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["There are 8732 samples in the dataset.\n"]}]},{"cell_type":"code","source":["label"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PyEbGY9sElK8","executionInfo":{"status":"ok","timestamp":1670164012731,"user_tz":-60,"elapsed":998,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"18099f27-e082-45f1-8ab0-5b754a58b8b2"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["signal.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Uu4417MEGIA","executionInfo":{"status":"ok","timestamp":1670164015333,"user_tz":-60,"elapsed":390,"user":{"displayName":"إسلام زيكوفتش","userId":"12934282606268707517"}},"outputId":"9384e1ab-a131-47fb-c569-b9012919e57b"},"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 64, 44])"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":[],"metadata":{"id":"b0zWZMWGiGQq"},"execution_count":null,"outputs":[]}]}